Sender: LSF System <lsfadmin@host453.jc.rl.ac.uk>
Subject: Job 3200631: </home/users/esmith88/roocs/dachar/lotus_script.py -p /badc/cmip6/data/CMIP6/CMIP/MPI-M/MPI-ESM1-2-HR/abrupt-4xCO2/r1i1p1f1/3hr/tas/gn/latest> in cluster <lotus> Done

Job </home/users/esmith88/roocs/dachar/lotus_script.py -p /badc/cmip6/data/CMIP6/CMIP/MPI-M/MPI-ESM1-2-HR/abrupt-4xCO2/r1i1p1f1/3hr/tas/gn/latest> was submitted from host <jasmin-sci5-panfs.ceda.ac.uk> by user <esmith88> in cluster <lotus>.
Job was executed on host(s) <host453.jc.rl.ac.uk>, in queue <short-serial>, as user <esmith88> in cluster <lotus>.
</home/users/esmith88> was used as the home directory.
</home/users/esmith88/roocs/dachar> was used as the working directory.
Started at Results reported on 
Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/home/users/esmith88/roocs/dachar/lotus_script.py -p /badc/cmip6/data/CMIP6/CMIP/MPI-M/MPI-ESM1-2-HR/abrupt-4xCO2/r1i1p1f1/3hr/tas/gn/latest
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   79.56 sec.
    Max Memory :                                 384 MB
    Average Memory :                             256.57 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   190061 MB
    Max Processes :                              3
    Max Threads :                                27
    Run time :                                   565 sec.
    Turnaround time :                            1352 sec.

The output (if any) follows:

ok


PS:

Read file <./outputs/badc/cmip6/data/CMIP6/CMIP/MPI-M/MPI-ESM1-2-HR/abrupt-4xCO2/r1i1p1f1/3hr/tas/gn/latest.err> for stderr output of this job.

